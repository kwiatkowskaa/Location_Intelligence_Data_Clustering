{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9faa90",
   "metadata": {},
   "source": [
    "# Location Intelligence Data Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394afa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed = 42\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import copy\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import geopandas as gpd\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30821607",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./DATA/google_places_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea63f8",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727286a5-b2ab-4649-81ed-8195ea5e06f9",
   "metadata": {},
   "source": [
    "#### Deleting duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['business_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb18f882-f01d-4c27-aa3f-6c9e638d60ad",
   "metadata": {},
   "source": [
    "#### Deleting NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f642b-4170-4c64-b546-c5d44562e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='any').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457950b2",
   "metadata": {},
   "source": [
    "#### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197bce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,13))\n",
    "\n",
    "for i, col in enumerate(['latitude', 'longitude', 'review_count', 'rating']):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    sns.boxplot(x = col, data = df, color='#0047AB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239244e5",
   "metadata": {},
   "source": [
    "##### Cutting off review_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064cd15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "review_count_count = df[\"review_count\"].value_counts().reset_index(name='count')\n",
    "review_count_count = review_count_count[review_count_count['count'] > 100]\n",
    "review_count_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43445b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['review_count'], bins=100, kde=True, color='#0047AB', edgecolor='black')\n",
    "\n",
    "highest_10_percent = df['review_count'].quantile(0.90)\n",
    "plt.axvline(x=highest_10_percent, color='red', linestyle='--')\n",
    "plt.title('Histogram of review_count')\n",
    "plt.xlabel('review_count')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "print(\"Value above which highest 10% of the data falls:\", highest_10_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['review_count'] > highest_10_percent, 'review_count'] = highest_10_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b671a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "sns.histplot(df['review_count'], bins=100, kde=True, color='#0047AB', edgecolor='black')\n",
    "plt.title('Histogram of review_count')\n",
    "plt.xlabel('review_count')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf796a41",
   "metadata": {},
   "source": [
    "#### Column removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960bc0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "for col in df.columns:\n",
    "    unique_values[col] = df[col].value_counts().shape[0]\n",
    "\n",
    "pd.DataFrame(unique_values, index=['unique value count']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e474e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['business_id', 'phone_number', 'name', 'full_address', 'place_id', 'place_link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722bb1f",
   "metadata": {},
   "source": [
    "Some categorical columns have values that are mostly unique to each record in the dataset. As a result, they will not be valuable for our model and we decided to remove them. \n",
    "\n",
    "Removed columns: \n",
    "* business_id\n",
    "* phone_number\t\n",
    "* name\t\n",
    "* full_address \n",
    "* place_id \n",
    "* place_link\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae569274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Monday_morning', 'Monday_afternoon', 'Monday_evening',\n",
    "                'Tuesday_morning', 'Tuesday_afternoon', 'Tuesday_evening',\n",
    "                'Wednesday_morning', 'Wednesday_afternoon', 'Wednesday_evening',\n",
    "                'Thursday_morning', 'Thursday_afternoon', 'Thursday_evening',\n",
    "                'Friday_morning', 'Friday_afternoon', 'Friday_evening',\n",
    "                'Saturday_morning', 'Saturday_afternoon', 'Saturday_evening',\n",
    "                'Sunday_morning', 'Sunday_afternoon', 'Sunday_evening'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0cb14c",
   "metadata": {},
   "source": [
    "From the EDA stage, we know that the columns related to business operating hours are highly correlated with each other, so we have decided to remove these columns as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9faba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49804802",
   "metadata": {},
   "source": [
    "The opening time is not important for our business problem, so we will delete all columns related to opening time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a0f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['geo_cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2872156",
   "metadata": {},
   "source": [
    "We decided to remove the geo_cluster column, which resulted from a previous clustering performed on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dfa3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4075db9",
   "metadata": {},
   "source": [
    "The \"state\" column indicates whether a given business is open or closed at the time of data collection and how much time remains until this changes. This column is not relevant for clustering, so we have decided to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0284071",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "for col in df.columns:\n",
    "    unique_values[col] = df[col].value_counts().shape[0]\n",
    "\n",
    "pd.DataFrame(unique_values, index=['unique value count']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f1bf5",
   "metadata": {},
   "source": [
    "##### website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_website'] = df['website'].apply(lambda x: 0 if x == 'Unknown' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da9fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_website_count = df[\"has_website\"].value_counts().reset_index(name='count')\n",
    "has_website_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e943d",
   "metadata": {},
   "source": [
    "##### country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country(city_country):\n",
    "    if ',' in city_country:\n",
    "        return city_country.split(',')[1].strip()\n",
    "    if '-' in city_country:\n",
    "        return city_country.split('-')[1].strip() \n",
    "    else:\n",
    "        return city_country\n",
    "\n",
    "def replace_two_letter_words(text):\n",
    "    pattern = r'\\b[A-Z]{2}\\b'\n",
    "    result = re.sub(pattern, 'USA', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961bbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = df['country'].apply(lambda x: extract_country(x))\n",
    "df['country'] = df['country'].apply(lambda x: replace_two_letter_words(x))\n",
    "country_count = df[\"country\"].value_counts().reset_index(name='count')\n",
    "country_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb96671",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = df['country'].value_counts()\n",
    "rare_categories = category_counts[category_counts < 10].index\n",
    "df['country'] = df['country'].apply(lambda x: 'Other' if x in rare_categories or x == 'Unknown' else x)\n",
    "country_count = df[\"country\"].value_counts().reset_index(name='count')\n",
    "country_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968170b0",
   "metadata": {},
   "source": [
    "This is still too many category (country) so we decide to not use column country as well as column city, but we will use more global column continent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2486061",
   "metadata": {},
   "source": [
    "##### timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a095333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timezone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59141c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['continent'] = df['timezone'].str.split('/').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c36e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['continent'] = df['continent'].apply(lambda x: 'America' if x == 'Pacific' else x)\n",
    "continent_count = df[\"continent\"].value_counts().reset_index(name='count')\n",
    "continent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018d2f13",
   "metadata": {},
   "source": [
    "##### types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d569b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove special chars and numbers\n",
    "    text = re.sub(\"[^A-Za-z]+\", \" \", text)\n",
    "    # return text in lower case and stripped of whitespaces\n",
    "    text = text.lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_types'] = df['types'].apply(lambda x: preprocess_text(x))\n",
    "df['cleaned_types']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c954e52",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b28fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.95)\n",
    "# fit_transform applies TF-IDF to clean texts - we save the array of vectors in X\n",
    "X = vectorizer.fit_transform(df['cleaned_types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_vecs = pca.fit_transform(X.toarray())\n",
    "x0 = pca_vecs[:, 0]\n",
    "x1 = pca_vecs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign clusters and pca vectors to our dataframe \n",
    "df['cluster_type'] = clusters\n",
    "df['x0'] = x0\n",
    "df['x1'] = x1\n",
    "df[['types', 'cluster_type', 'x0', 'x1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca0e56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the keywords for each centroid of the KMeans\"\"\"\n",
    "    df = pd.DataFrame(X.todense()).groupby(clusters).mean() # groups the TF-IDF vector by cluster\n",
    "    terms = vectorizer.get_feature_names_out() # access tf-idf terms\n",
    "    for i,r in df.iterrows():\n",
    "        print('\\nCluster {}'.format(i))\n",
    "        print(','.join([terms[t] for t in np.argsort(r)[-n_terms:]])) # for each row of the dataframe, find the n terms that have the highest tf idf score\n",
    "            \n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38b123",
   "metadata": {},
   "source": [
    "#### Universal Sentence Encoder (USE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow tensorflow-hub scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder model\n",
    "use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "def embed_text(text, model):\n",
    "    embeddings = model([text])\n",
    "    return embeddings.numpy().squeeze()\n",
    "\n",
    "embeddings = []\n",
    "for text in df['cleaned_types']:\n",
    "    embeddings.append(embed_text(text, use_model))\n",
    "\n",
    "X = np.array(embeddings)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "def get_top_keywords(n_terms):\n",
    "    \"\"\"This function returns the top keywords for each cluster.\"\"\"\n",
    "    df['cluster'] = clusters\n",
    "    for i in range(len(set(clusters))):\n",
    "        cluster_texts = df[df['cluster'] == i]['cleaned_types']\n",
    "        all_words = ' '.join(cluster_texts).lower()\n",
    "        all_words = re.findall(r'\\b\\w+\\b', all_words)  \n",
    "        common_words = Counter(all_words).most_common(n_terms)\n",
    "        top_keywords = [word for word, count in common_words]\n",
    "        print(f'\\nCluster {i}')\n",
    "        print(', '.join(top_keywords))\n",
    "\n",
    "get_top_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d8e314",
   "metadata": {},
   "source": [
    "We have decided to use the TfidfVectorizer method based on a comparison of the top words in each group. TfidfVectorizer better characterizes the types of places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33428a8d-3580-41c7-a5c8-e08d57a4f584",
   "metadata": {},
   "source": [
    "#####  latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4821f6e-a487-4419-8068-27be081076d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_plots(X, max_k=10):\n",
    "\n",
    "    score = []\n",
    "    score_kmeans_s = []\n",
    "    score_kmeans_c = []\n",
    "    score_kmeans_d = []\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        kmeans = KMeans(n_clusters=k, random_state= 101)\n",
    "        predictions = kmeans.fit_predict(X)\n",
    "\n",
    "        score.append(kmeans.score(X))\n",
    "        score_kmeans_s.append(silhouette_score(X, kmeans.labels_, metric='euclidean'))\n",
    "        score_kmeans_c.append(calinski_harabasz_score(X, kmeans.labels_))\n",
    "        score_kmeans_d.append(davies_bouldin_score(X, predictions))\n",
    "\n",
    "    list_scores = [score, score_kmeans_s, score_kmeans_c, score_kmeans_d] \n",
    "    # Elbow Method plot\n",
    "    list_title = ['Within-cluster sum of squares', 'Silhouette Score', 'Calinski Harabasz', 'Davies Bouldin'] \n",
    "    for i in range(len(list_scores)):\n",
    "        x_ticks = list(range(2, len(list_scores[i]) + 2))\n",
    "        plt.plot(x_ticks, list_scores[i], 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(list_title[i])\n",
    "        plt.title('Optimal k')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a039c-a139-4595-b36a-3544646c972b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_geografics  = df[[\"longitude\", \"latitude\"]]\n",
    "metrics_plots(df_geografics, max_k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3eb0c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "def metrics_plots(X, max_k=10):\n",
    "    score = []\n",
    "    score_kmedoids_s = []\n",
    "    score_kmedoids_c = []\n",
    "    score_kmedoids_d = []\n",
    "    \n",
    "    distance_matrix = haversine_distances(np.radians(df[[\"longitude\", \"latitude\"]]))\n",
    "    \n",
    "    for k in range(2, max_k):\n",
    "        \n",
    "        kmedoids = KMedoids(n_clusters=k, metric='precomputed', random_state=42)        \n",
    "        kmedoids.fit(distance_matrix)\n",
    "\n",
    "        score.append(-kmedoids.score(X))  \n",
    "        score_kmedoids_s.append(silhouette_score(X, predictions, metric='euclidean'))\n",
    "        score_kmedoids_c.append(calinski_harabasz_score(X, predictions))\n",
    "        score_kmedoids_d.append(davies_bouldin_score(X, predictions))\n",
    "\n",
    "    list_scores = [score, score_kmedoids_s, score_kmedoids_c, score_kmedoids_d]\n",
    "    list_title = ['Negative Total Distance', 'Silhouette Score', 'Calinski Harabasz', 'Davies Bouldin']\n",
    "\n",
    "    for i in range(len(list_scores)):\n",
    "        x_ticks = list(range(2, len(list_scores[i]) + 2))\n",
    "        plt.plot(x_ticks, list_scores[i], 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(list_title[i])\n",
    "        plt.title('Optimal k')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ee61620",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KMedoids' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_geografics  \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m----> 2\u001b[0m metrics_plots(df_geografics, max_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 16\u001b[0m, in \u001b[0;36mmetrics_plots\u001b[1;34m(X, max_k)\u001b[0m\n\u001b[0;32m     13\u001b[0m kmedoids \u001b[38;5;241m=\u001b[39m KMedoids(n_clusters\u001b[38;5;241m=\u001b[39mk, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)        \n\u001b[0;32m     14\u001b[0m kmedoids\u001b[38;5;241m.\u001b[39mfit(distance_matrix)\n\u001b[1;32m---> 16\u001b[0m score\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mkmedoids\u001b[38;5;241m.\u001b[39mscore(X))  \n\u001b[0;32m     17\u001b[0m score_kmedoids_s\u001b[38;5;241m.\u001b[39mappend(silhouette_score(X, predictions, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     18\u001b[0m score_kmedoids_c\u001b[38;5;241m.\u001b[39mappend(calinski_harabasz_score(X, predictions))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KMedoids' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "df_geografics  = df[[\"longitude\", \"latitude\"]]\n",
    "metrics_plots(df_geografics, max_k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1ee404-4441-4dbe-8436-d7a291e04a7d",
   "metadata": {},
   "source": [
    "* 'Within-cluster sum of squares' -> k ~ 4\n",
    "\n",
    "* 'Silhouette Score' -> k = 4\n",
    "\n",
    "* 'Calinski Harabasz' -> k = 5\n",
    "\n",
    "* 'Davies Bouldin' -> k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b21c2-d490-4bc8-9255-330adacdfb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids = KMedoids(n_clusters=4, random_state=0)\n",
    "kmedoids.fit(df_geografics)\n",
    "y_kmedoids = kmedoids.predict(df_geografics)\n",
    "cent = kmedoids.cluster_centers_\n",
    "cent = pd.DataFrame(cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7c260-d7f5-4ab6-822a-10e14f9eb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df_geografics, geometry=gpd.points_from_xy(df['longitude'], df['latitude']))\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "world = world[(world.name != \"Antarctica\")]\n",
    "\n",
    "world.plot(figsize=(15, 7), color='lightgray', edgecolor='white')\n",
    "\n",
    "gdf.plot(ax=plt.gca(), marker='o', column=y_kmedoids, markersize=9)\n",
    "\n",
    "gdf2 = gpd.GeoDataFrame(cent, geometry=gpd.points_from_xy(cent[0], cent[1]))\n",
    "gdf2.plot(ax=plt.gca(), marker='x', markersize=50, color = \"hotpink\")\n",
    "\n",
    "plt.title('Businesses on world map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced0843",
   "metadata": {},
   "source": [
    "### Train, Validation and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./DATA/google_places_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev, X_test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "X_train, X_val = train_test_split(X_dev, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeeda39-2b96-4674-b2b1-12a8675f560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lat_long = copy.deepcopy(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7807f76-12cb-4f8c-aba0-90de5f64cba8",
   "metadata": {},
   "source": [
    "### Feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71386883-2ed9-4da6-bfb4-867f84c0bce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutOffReviewCount(data):\n",
    "    highest_10_percent = data['review_count'].quantile(0.90)\n",
    "    data.loc[df['review_count'] > highest_10_percent, 'review_count'] = highest_10_percent\n",
    "    return data\n",
    "\n",
    "def dropDuplicatedRows(data):\n",
    "    data = data.drop_duplicates(subset=['business_id'])\n",
    "    return data\n",
    "\n",
    "def dropNAValues(data):\n",
    "    data = data.dropna(how='any').reset_index(drop = True)\n",
    "    return data\n",
    "\n",
    "def hasWebsite(data):\n",
    "    data['has_website'] = data['website'].apply(lambda x: 0 if x == 'Unknown' else 1)\n",
    "    data = data.drop(columns=['website'])\n",
    "    return data\n",
    "\n",
    "def verifiedToInt(data):\n",
    "    data['verified'] = data['verified'].astype(int)\n",
    "    return data\n",
    "\n",
    "def dropCountryandCityColumn(data):\n",
    "    data = data.drop(columns=['country'])\n",
    "    data = data.drop(columns=['city'])\n",
    "    return data\n",
    "\n",
    "def continentColumn(data):\n",
    "    data['continent'] = data['timezone'].str.split('/').str[0]\n",
    "    data['continent'] = data['continent'].apply(lambda x: 'America' if x == 'Pacific' else x)\n",
    "    data = data.drop(columns=['timezone'])\n",
    "    return data\n",
    "\n",
    "def dropUniqueColumns(data):\n",
    "    data = data.drop(columns=['business_id', 'phone_number', 'name', 'full_address', 'place_id', 'place_link'])\n",
    "    return data\n",
    "\n",
    "def dropDayTimeColumns(data):\n",
    "    data = data.drop(columns=['Monday_morning', 'Monday_afternoon', 'Monday_evening',\n",
    "                'Tuesday_morning', 'Tuesday_afternoon', 'Tuesday_evening',\n",
    "                'Wednesday_morning', 'Wednesday_afternoon', 'Wednesday_evening',\n",
    "                'Thursday_morning', 'Thursday_afternoon', 'Thursday_evening',\n",
    "                'Friday_morning', 'Friday_afternoon', 'Friday_evening',\n",
    "                'Saturday_morning', 'Saturday_afternoon', 'Saturday_evening',\n",
    "                'Sunday_morning', 'Sunday_afternoon', 'Sunday_evening',\n",
    "                'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "    return data\n",
    "\n",
    "def dropGeoClusterAndState(data):\n",
    "    data = data.drop(columns=['geo_cluster', 'state'])\n",
    "    return data\n",
    "\n",
    "\n",
    "feature_engineering_pipeline = Pipeline([\n",
    "    ('cutOffReviewCount', FunctionTransformer(cutOffReviewCount)),\n",
    "    ('dropDuplicatedRows', FunctionTransformer(dropDuplicatedRows)),\n",
    "    ('dropNAValues', FunctionTransformer(dropNAValues)),\n",
    "    ('hasWebsite', FunctionTransformer(hasWebsite)),\n",
    "    ('verifiedToInt', FunctionTransformer(verifiedToInt)),\n",
    "    ('dropCountryandCityColumn', FunctionTransformer(dropCountryandCityColumn)),\n",
    "    ('continentColumn', FunctionTransformer(continentColumn)),\n",
    "    ('dropUniqueColumns', FunctionTransformer(dropUniqueColumns)),\n",
    "    ('dropDayTimeColumns', FunctionTransformer(dropDayTimeColumns)),\n",
    "    ('dropGeoClusterAndState', FunctionTransformer(dropGeoClusterAndState))    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5761a6-b013-413c-a16b-221b75f2e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_engineering_pipeline.transform(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c314ce0-618d-4502-bc09-0d7ccd240e76",
   "metadata": {},
   "source": [
    "### Feature engineering pipeline with fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2f249-a344-48ee-a504-803d12bf2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMedoidsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=4, random_state=0):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.medoids_model = None\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        X_geografics = data[[\"longitude\", \"latitude\"]]\n",
    "        self.medoids_model = KMedoids(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        self.medoids_model.fit(X_geografics)\n",
    "        # y_kmedoids =  self.medoids_model.predict(X_geografics)\n",
    "        # data[\"geo_cluster\"] = y_kmedoids\n",
    "        # return data\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, y=None):\n",
    "        X_geografics = data[[\"longitude\", \"latitude\"]]\n",
    "        y_kmedoids = self.medoids_model.predict(X_geografics)\n",
    "        data = data.drop(columns=[\"longitude\", \"latitude\"])\n",
    "        data[\"geo_cluster\"] = y_kmedoids\n",
    "        return data\n",
    "\n",
    "class cutOffReviewCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, highest_10_percent=0):\n",
    "        self.highest_10_percent = highest_10_percent\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        self.highest_10_percent = data['review_count'].quantile(0.90)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data, y=None):\n",
    "        data.loc[data['review_count'] > self.highest_10_percent, 'review_count'] = self.highest_10_percent\n",
    "        return data\n",
    "\n",
    "feature_engineering_fitted_pipeline = Pipeline([\n",
    "    ('KMedoidsTransformer', KMedoidsTransformer()),\n",
    "    ('cutOffReviewCountTransformer', cutOffReviewCountTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672aeac-16e6-4de9-8290-608ef34005c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_engineering_fitted_pipeline.fit(X_train)\n",
    "X_train = feature_engineering_fitted_pipeline.transform(X_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c876e",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733262a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"review_count\", \"rating\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"continent\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"encoder\", OneHotEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    "\n",
    ")\n",
    "\n",
    "pipeline_preprocessing = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pipeline_preprocessing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faad4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns=pipeline_preprocessing.named_steps['preprocessor'].get_feature_names_out())\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372a696-7765-4d7a-8416-569e93f70a2b",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd201b-7a7f-4dac-8a86-fcab272ed810",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=['remainder__types'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402597eb-463f-43b4-95c2-51c9a00eee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_plots(X, max_k=10):\n",
    "\n",
    "    score = []\n",
    "    score_kmeans_s = []\n",
    "    score_kmeans_c = []\n",
    "    score_kmeans_d = []\n",
    "\n",
    "    for k in range(2, max_k):\n",
    "        kmeans = KMeans(n_clusters=k, random_state= 101)\n",
    "        predictions = kmeans.fit_predict(X)\n",
    "        # Calculate cluster validation metrics and append to lists of metrics\n",
    "        score.append(kmeans.score(X))\n",
    "        score_kmeans_s.append(silhouette_score(X, kmeans.labels_, metric='euclidean'))\n",
    "        score_kmeans_c.append(calinski_harabasz_score(X, kmeans.labels_))\n",
    "        score_kmeans_d.append(davies_bouldin_score(X, predictions))\n",
    "\n",
    "    list_scores = [score, score_kmeans_s, score_kmeans_c, score_kmeans_d] \n",
    "    # Elbow Method plot\n",
    "    list_title = ['Within-cluster sum of squares', 'Silhouette Score', 'Calinski Harabasz', 'Davies Bouldin'] \n",
    "    for i in range(len(list_scores)):\n",
    "        x_ticks = list(range(2, len(list_scores[i]) + 2))\n",
    "        plt.plot(x_ticks, list_scores[i], 'bx-')\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(list_title[i])\n",
    "        plt.title('Optimal k')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16670376-ec1c-46a9-aecf-1a0a2935455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_plots(X_train, max_k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81d73b8-8b81-4237-be63-3c57382b6cac",
   "metadata": {},
   "source": [
    "'Within-cluster sum of squares' -> k ~ 7\n",
    "\n",
    "'Silhouette Score' -> k ~ 5\n",
    "\n",
    "'Calinski Harabasz' -> k = 4\n",
    "\n",
    "'Davies Bouldin' -> k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3cbad-2acf-4d69-860f-fe786bc7ed9c",
   "metadata": {},
   "source": [
    "#### Results functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52278337-2fb5-4912-acb2-6d45176b1109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_geo(data):\n",
    "    X_train_lat_long = feature_engineering_pipeline.transform(data)\n",
    "    X_train_lat_long = pipeline_preprocessing.fit_transform(X_train_lat_long)\n",
    "    X_train_lat_long = pd.DataFrame(X_train_lat_long, columns=pipeline_preprocessing.named_steps['preprocessor'].get_feature_names_out())\n",
    "    X_train_lat_long = X_train_lat_long[[\"remainder__longitude\", \"remainder__latitude\"]]\n",
    "    return X_train_lat_long\n",
    "\n",
    "def drawMap(data, labels):\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(X_train_lat_long['remainder__longitude'], X_train_lat_long['remainder__latitude']))\n",
    "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "    world = world[(world.name != \"Antarctica\")]\n",
    "    world.plot(figsize=(15, 7), color='lightgray', edgecolor='white')\n",
    "    gdf.plot(ax=plt.gca(), marker='o', column=labels, markersize=9, legend=True)\n",
    "    plt.title('Businesses on world map')\n",
    "    plt.show()\n",
    "\n",
    "def CountClasters(labels):\n",
    "    df = pd.DataFrame({\"labels\": labels})\n",
    "    value_counts = df[\"labels\"].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis')\n",
    "    plt.title('Distribution of Clusters')\n",
    "    plt.xlabel('Cluster number')\n",
    "    plt.ylabel('Number of Bisinesses')\n",
    "    plt.show()\n",
    "\n",
    "def calculateScores(data, labels):\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    calinski_score = calinski_harabasz_score(data, labels)\n",
    "    davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    Scores = {\n",
    "    'Score name': ['Silhouette Score', 'Calinski-Harabaz Index', 'Davies-Bouldin Index'],\n",
    "    'score value': [silhouette_avg, calinski_score, davies_bouldin]\n",
    "    }\n",
    "\n",
    "    df_scores = pd.DataFrame(Scores)\n",
    "    return df_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcdc78-df88-40fb-b4fd-31828435116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lat_long = data_geo(X_train_lat_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6d000-4fd1-4a3d-a9fd-13c15107007c",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adcde9-f46b-4c8b-b317-8cbb7e272fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "labels = kmeans.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3d2099-b103-4cbb-a711-f50dd774ebbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawMap(X_train, labels)\n",
    "CountClasters(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa882c97-42d4-42be-9567-5018a1ef8324",
   "metadata": {},
   "source": [
    "#### KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b63be3-1371-4600-b4e5-a68462a8bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "medoids_model = KMedoids(n_clusters=5, random_state=0)\n",
    "medoids_model.fit(X_train)\n",
    "labels =  medoids_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe4e99-f276-4220-addb-e69823280b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawMap(X_train, labels)\n",
    "CountClasters(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c2e19-2eec-4583-8f84-19b12db9c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateScores(X_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e0c5b-de04-4292-86cf-3ca66a09d09f",
   "metadata": {},
   "source": [
    "#### Single Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890148c3-f676-4bd8-8eaf-3a11e52e1182",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X_train, method='single')\n",
    "plt.figure(figsize=(10, 5), dpi= 200, facecolor='w', edgecolor='k')\n",
    "hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa12b3-04c2-4541-9f95-1982d4024182",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 30), dpi= 200, facecolor='w', edgecolor='k')\n",
    "hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a7f398-4afc-4595-b06c-7d209f5d9c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=None, linkage='single', distance_threshold=1.4)\n",
    "labels = model.fit_predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df448953-6d25-4eab-8ebe-433dcbddb060",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawMap(X_train, labels)\n",
    "CountClasters(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc45c3c-47a1-4583-bc2f-c11dab99fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateScores(X_train, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c0dc2-4cb1-46ce-93f1-57964a86eacf",
   "metadata": {},
   "source": [
    "#### Complete Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0132d5a-784e-4c84-82ce-4d956f7d72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = hierarchy.linkage(X_train, method='complete')\n",
    "plt.figure(figsize=(10, 20), dpi= 200, facecolor='w', edgecolor='k')\n",
    "hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127e8f4-7b63-4bef-8075-526c0b88bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=None, linkage='complete', distance_threshold=4)\n",
    "labels = model.fit_predict(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdbd0b-bc49-4f81-9499-3dce5ffbdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "drawMap(X_train, labels)\n",
    "CountClasters(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d519dc-5b82-4000-aa8f-49ac179fdaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculateScores(X_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a95c8e-f6f9-41d2-88e8-c0018c78562b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
